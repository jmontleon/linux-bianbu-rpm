From 9f5dd2cc8a82c12b9e6afa28d02b53a03c24c302 Mon Sep 17 00:00:00 2001
From: huzhen <george.hu@spacemit.com>
Date: Mon, 8 May 2023 15:30:53 +0800
Subject: [PATCH 0018/1206] spi: adding the driver for Designware enhanced spi
 controller

Change-Id: I2e86ba09d14331bec4f9878c8f7e453c12831622
---
 arch/riscv/boot/dts/spacemit/k1-pro.dtsi |   19 +
 arch/riscv/configs/k1-pro_fpga_defconfig |    3 +
 drivers/spi/Kconfig                      |    6 +
 drivers/spi/Makefile                     |    1 +
 drivers/spi/spi-dw-espi.c                | 1237 ++++++++++++++++++++++
 drivers/spi/spi-dw-espi.h                |  325 ++++++
 drivers/spi/spi-dw-mmio-ext.c            |  171 +++
 7 files changed, 1762 insertions(+)
 create mode 100755 drivers/spi/spi-dw-espi.c
 create mode 100755 drivers/spi/spi-dw-espi.h
 create mode 100755 drivers/spi/spi-dw-mmio-ext.c

diff --git a/arch/riscv/boot/dts/spacemit/k1-pro.dtsi b/arch/riscv/boot/dts/spacemit/k1-pro.dtsi
index bb35e791495e..1546555f4490 100644
--- a/arch/riscv/boot/dts/spacemit/k1-pro.dtsi
+++ b/arch/riscv/boot/dts/spacemit/k1-pro.dtsi
@@ -164,6 +164,12 @@ pll_clk_400m: clock-pll_clk_400m {
 			clock-output-names = "pll_clk_400m";
 		};
 
+               pll_clk_qspi: clock-pll_spi {
+		       #clock-cells = <0>;
+                       compatible = "fixed-clock";
+                       clock-frequency = <6250000>;
+                       clock-output-names = "pll_qspi";
+               };
 	};
 	soc {
 		compatible = "simple-bus";
@@ -267,6 +273,19 @@ core3_tcm@60000 {
 				pool;
 			};
 		};
+
+		qspi0: spi@2ce00000 {
+                       compatible = "snps,dwc-ssi-1.02a";
+                       reg = <0x0 0x2ce00000 0x0 0x4000>;
+                       interrupt-parent = <&intc>;
+                       interrupts = <96>;
+                       clocks = <&pll_clk_qspi>;
+                       resets = <&reset RESET_QSPI0>;
+                       #address-cells = <1>;
+                       #size-cells = <0>;
+                       status = "disabled";
+               };
+
 	};
 };
 
diff --git a/arch/riscv/configs/k1-pro_fpga_defconfig b/arch/riscv/configs/k1-pro_fpga_defconfig
index 5921100f6b24..6a711b3074d2 100644
--- a/arch/riscv/configs/k1-pro_fpga_defconfig
+++ b/arch/riscv/configs/k1-pro_fpga_defconfig
@@ -62,6 +62,8 @@ CONFIG_SERIAL_OF_PLATFORM=y
 CONFIG_SERIAL_EARLYCON_RISCV_SBI=y
 CONFIG_VIRTIO_CONSOLE=y
 # CONFIG_HW_RANDOM is not set
+CONFIG_SPI=y
+CONFIG_SPI_DESIGNWARE_EXT=y
 # CONFIG_HWMON is not set
 CONFIG_MFD_SYSCON=y
 # CONFIG_VGA_CONSOLE is not set
@@ -73,6 +75,7 @@ CONFIG_UDMABUF=y
 # CONFIG_VIRTIO_MENU is not set
 # CONFIG_VHOST_MENU is not set
 # CONFIG_IOMMU_SUPPORT is not set
+CONFIG_RESET_CONTROLLER=y
 # CONFIG_NVMEM is not set
 CONFIG_EXT4_FS=y
 CONFIG_EXT4_FS_POSIX_ACL=y
diff --git a/drivers/spi/Kconfig b/drivers/spi/Kconfig
index f51f9466e518..3bf15dc15cd4 100644
--- a/drivers/spi/Kconfig
+++ b/drivers/spi/Kconfig
@@ -323,6 +323,12 @@ config SPI_DAVINCI
 	help
 	  SPI master controller for DaVinci/DA8x/OMAP-L/AM1x SPI modules.
 
+config SPI_DESIGNWARE_EXT
+	tristate "DesignWare enhanced SPI controller core support"
+        imply SPI_MEM
+	help
+	  general driver for enhanced SPI controller core from  DesignWare
+
 config SPI_DESIGNWARE
 	tristate "DesignWare SPI controller core support"
 	imply SPI_MEM
diff --git a/drivers/spi/Makefile b/drivers/spi/Makefile
index aea5e54de195..fb9c1636b35f 100644
--- a/drivers/spi/Makefile
+++ b/drivers/spi/Makefile
@@ -46,6 +46,7 @@ obj-$(CONFIG_SPI_COLDFIRE_QSPI)		+= spi-coldfire-qspi.o
 obj-$(CONFIG_SPI_CS42L43)		+= spi-cs42l43.o
 obj-$(CONFIG_SPI_DAVINCI)		+= spi-davinci.o
 obj-$(CONFIG_SPI_DLN2)			+= spi-dln2.o
+obj-$(CONFIG_SPI_DESIGNWARE_EXT)	+= spi-dw-mmio-ext.o spi-dw-espi.o
 obj-$(CONFIG_SPI_DESIGNWARE)		+= spi-dw.o
 spi-dw-y				:= spi-dw-core.o
 spi-dw-$(CONFIG_SPI_DW_DMA)		+= spi-dw-dma.o
diff --git a/drivers/spi/spi-dw-espi.c b/drivers/spi/spi-dw-espi.c
new file mode 100755
index 000000000000..0ba9100af3ad
--- /dev/null
+++ b/drivers/spi/spi-dw-espi.c
@@ -0,0 +1,1237 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Designware SPI core controller driver
+ *
+ * Copyright (c) 2023, spacemit Corporation.
+ *
+ * base on design-ware spi-core driver(spi-dw-xxx.c)
+ */
+
+#include <linux/bitfield.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/preempt.h>
+#include <linux/highmem.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/spi/spi.h>
+#include <linux/spi/spi-mem.h>
+#include <linux/string.h>
+#include <linux/of.h>
+
+#include "spi-dw-espi.h"
+
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+#endif
+
+/* Slave spi_device related */
+struct dw_spi_chip_data {
+	u32 cr0;
+	u32 rx_sample_dly;	/* RX sample delay */
+};
+
+#ifdef CONFIG_DEBUG_FS
+
+#define DW_SPI_DBGFS_REG(_name, _off)	\
+{					\
+	.name = _name,			\
+	.offset = _off,			\
+}
+
+static const struct debugfs_reg32 dw_spi_dbgfs_regs[] = {
+	DW_SPI_DBGFS_REG("CTRLR0", DW_SPI_CTRLR0),
+	DW_SPI_DBGFS_REG("CTRLR1", DW_SPI_CTRLR1),
+	DW_SPI_DBGFS_REG("SSIENR", DW_SPI_SSIENR),
+	DW_SPI_DBGFS_REG("SER", DW_SPI_SER),
+	DW_SPI_DBGFS_REG("BAUDR", DW_SPI_BAUDR),
+	DW_SPI_DBGFS_REG("TXFTLR", DW_SPI_TXFTLR),
+	DW_SPI_DBGFS_REG("RXFTLR", DW_SPI_RXFTLR),
+	DW_SPI_DBGFS_REG("TXFLR", DW_SPI_TXFLR),
+	DW_SPI_DBGFS_REG("RXFLR", DW_SPI_RXFLR),
+	DW_SPI_DBGFS_REG("SR", DW_SPI_SR),
+	DW_SPI_DBGFS_REG("IMR", DW_SPI_IMR),
+	DW_SPI_DBGFS_REG("ISR", DW_SPI_ISR),
+	DW_SPI_DBGFS_REG("DMACR", DW_SPI_DMACR),
+	DW_SPI_DBGFS_REG("DMATDLR", DW_SPI_DMATDLR),
+	DW_SPI_DBGFS_REG("DMARDLR", DW_SPI_DMARDLR),
+	DW_SPI_DBGFS_REG("RX_SAMPLE_DLY", DW_SPI_RX_SAMPLE_DLY),
+};
+
+static int dw_spi_debugfs_init(struct dw_spi *dws)
+{
+	char name[32];
+
+	snprintf(name, 32, "dw_spi%d", dws->master->bus_num);
+	dws->debugfs = debugfs_create_dir(name, NULL);
+	if (!dws->debugfs)
+		return -ENOMEM;
+
+	dws->regset.regs = dw_spi_dbgfs_regs;
+	dws->regset.nregs = ARRAY_SIZE(dw_spi_dbgfs_regs);
+	dws->regset.base = dws->regs;
+	debugfs_create_regset32("registers", 0400, dws->debugfs, &dws->regset);
+
+	return 0;
+}
+
+static void dw_spi_debugfs_remove(struct dw_spi *dws)
+{
+	debugfs_remove_recursive(dws->debugfs);
+}
+
+#else
+static inline int dw_spi_debugfs_init(struct dw_spi *dws)
+{
+	return 0;
+}
+
+static inline void dw_spi_debugfs_remove(struct dw_spi *dws)
+{
+}
+#endif /* CONFIG_DEBUG_FS */
+
+void dw_spi_ext_set_cs(struct spi_device *spi, bool enable)
+{
+	struct dw_spi *dws = spi_controller_get_devdata(spi->controller);
+	bool cs_high = !!(spi->mode & SPI_CS_HIGH);
+
+	/*
+	 * DW SPI controller demands any native CS being set in order to
+	 * proceed with data transfer. So in order to activate the SPI
+	 * communications we must set a corresponding bit in the Slave
+	 * Enable register no matter whether the SPI core is configured to
+	 * support active-high or active-low CS level.
+	 */
+	if (cs_high == enable)
+		dw_writel(dws, DW_SPI_SER, BIT(spi->chip_select));
+	else
+		dw_writel(dws, DW_SPI_SER, 0);
+}
+
+/* Return the max entries we can fill into tx fifo */
+static inline u32 dw_spi_tx_max(struct dw_spi *dws)
+{
+	u32 tx_room, rxtx_gap;
+
+	tx_room = dws->fifo_len - dw_readl(dws, DW_SPI_TXFLR);
+
+	/*
+	 * Another concern is about the tx/rx mismatch, we
+	 * though to use (dws->fifo_len - rxflr - txflr) as
+	 * one maximum value for tx, but it doesn't cover the
+	 * data which is out of tx/rx fifo and inside the
+	 * shift registers. So a control from sw point of
+	 * view is taken.
+	 */
+	rxtx_gap = dws->fifo_len - (dws->rx_len - dws->tx_len);
+
+	return min3((u32)dws->tx_len, tx_room, rxtx_gap);
+}
+
+/* Return the max entries we should read out of rx fifo */
+static inline u32 dw_spi_rx_max(struct dw_spi *dws)
+{
+	return min_t(u32, dws->rx_len, dw_readl(dws, DW_SPI_RXFLR));
+}
+
+static void dw_writer(struct dw_spi *dws)
+{
+	u32 max = dw_spi_tx_max(dws);
+	u32 txw = 0;
+
+	while (max--) {
+		if (dws->tx) {
+			if (dws->n_bytes == 1)
+				txw = *(u8 *)(dws->tx);
+			else if (dws->n_bytes == 2)
+				txw = *(u16 *)(dws->tx);
+			else
+				txw = *(u32 *)(dws->tx);
+
+			dws->tx += dws->n_bytes;
+		}
+		dw_write_io_reg(dws, DW_SPI_DR, txw);
+		--dws->tx_len;
+	}
+}
+
+static void dw_reader(struct dw_spi *dws)
+{
+	u32 max = dw_spi_rx_max(dws);
+	u32 rxw;
+
+	while (max--) {
+		rxw = dw_read_io_reg(dws, DW_SPI_DR);
+		if (dws->rx) {
+			if (dws->n_bytes == 1)
+				*(u8 *)(dws->rx) = rxw;
+			else if (dws->n_bytes == 2)
+				*(u16 *)(dws->rx) = rxw;
+			else
+				*(u32 *)(dws->rx) = rxw;
+
+			dws->rx += dws->n_bytes;
+		}
+		--dws->rx_len;
+	}
+}
+
+int dw_spi_ext_check_status(struct dw_spi *dws, bool raw)
+{
+	u32 irq_status;
+	int ret = 0;
+
+	if (raw)
+		irq_status = dw_readl(dws, DW_SPI_RISR);
+	else
+		irq_status = dw_readl(dws, DW_SPI_ISR);
+
+	if (irq_status & DW_SPI_INT_RXOI) {
+		dev_err(&dws->master->dev, "RX FIFO overflow detected\n");
+		ret = -EIO;
+	}
+
+	if (irq_status & DW_SPI_INT_RXUI) {
+		dev_err(&dws->master->dev, "RX FIFO underflow detected\n");
+		ret = -EIO;
+	}
+
+	if (irq_status & DW_SPI_INT_TXOI) {
+		dev_err(&dws->master->dev, "TX FIFO overflow detected\n");
+		ret = -EIO;
+	}
+
+	/* Generically handle the erroneous situation */
+	if (ret) {
+		dw_spi_reset_chip(dws);
+		if (dws->master->cur_msg)
+			dws->master->cur_msg->status = ret;
+	}
+
+	return ret;
+}
+
+static irqreturn_t dw_spi_transfer_handler(struct dw_spi *dws)
+{
+	u16 irq_status = dw_readl(dws, DW_SPI_ISR);
+
+	if (dw_spi_ext_check_status(dws, false)) {
+		spi_finalize_current_transfer(dws->master);
+		return IRQ_HANDLED;
+	}
+
+	/*
+	 * Read data from the Rx FIFO every time we've got a chance executing
+	 * this method. If there is nothing left to receive, terminate the
+	 * procedure. Otherwise adjust the Rx FIFO Threshold level if it's a
+	 * final stage of the transfer. By doing so we'll get the next IRQ
+	 * right when the leftover incoming data is received.
+	 */
+	dw_reader(dws);
+	if (!dws->rx_len) {
+		dw_spi_mask_intr(dws, 0xff);
+		spi_finalize_current_transfer(dws->master);
+	} else if (dws->rx_len <= dw_readl(dws, DW_SPI_RXFTLR)) {
+		dw_writel(dws, DW_SPI_RXFTLR, dws->rx_len - 1);
+	}
+
+	/*
+	 * Send data out if Tx FIFO Empty IRQ is received. The IRQ will be
+	 * disabled after the data transmission is finished so not to
+	 * have the TXE IRQ flood at the final stage of the transfer.
+	 */
+	if (irq_status & DW_SPI_INT_TXEI) {
+		dw_writer(dws);
+		if (!dws->tx_len)
+			dw_spi_mask_intr(dws, DW_SPI_INT_TXEI);
+	}
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t dw_spi_ext_irq(int irq, void *dev_id)
+{
+	struct spi_controller *master = dev_id;
+	struct dw_spi *dws = spi_controller_get_devdata(master);
+	u16 irq_status = dw_readl(dws, DW_SPI_ISR) & DW_SPI_INT_MASK;
+
+	if (!irq_status)
+		return IRQ_NONE;
+
+	if (!master->cur_msg) {
+		dw_spi_mask_intr(dws, 0xff);
+		return IRQ_HANDLED;
+	}
+
+	return dws->transfer_handler(dws);
+}
+
+static u32 dw_spi_prepare_cr0(struct dw_spi *dws, struct spi_device *spi)
+{
+	u32 cr0 = 0;
+
+	if (dw_spi_ip_is(dws, PSSI)) {
+		/* CTRLR0[ 5: 4] Frame Format */
+		cr0 |= FIELD_PREP(DW_PSSI_CTRLR0_FRF_MASK, DW_SPI_CTRLR0_FRF_MOTO_SPI);
+
+		/*
+		 * SPI mode (SCPOL|SCPH)
+		 * CTRLR0[ 6] Serial Clock Phase
+		 * CTRLR0[ 7] Serial Clock Polarity
+		 */
+		if (spi->mode & SPI_CPOL)
+			cr0 |= DW_PSSI_CTRLR0_SCPOL;
+		if (spi->mode & SPI_CPHA)
+			cr0 |= DW_PSSI_CTRLR0_SCPHA;
+
+		/* CTRLR0[11] Shift Register Loop */
+		if (spi->mode & SPI_LOOP)
+			cr0 |= DW_PSSI_CTRLR0_SRL;
+	} else {
+		/* CTRLR0[ 7: 6] Frame Format */
+		cr0 |= FIELD_PREP(DW_HSSI_CTRLR0_FRF_MASK, DW_SPI_CTRLR0_FRF_MOTO_SPI);
+
+		/*
+		 * SPI mode (SCPOL|SCPH)
+		 * CTRLR0[ 8] Serial Clock Phase
+		 * CTRLR0[ 9] Serial Clock Polarity
+		 */
+		if (spi->mode & SPI_CPOL)
+			cr0 |= DW_HSSI_CTRLR0_SCPOL;
+		if (spi->mode & SPI_CPHA)
+			cr0 |= DW_HSSI_CTRLR0_SCPHA;
+
+		/* CTRLR0[13] Shift Register Loop */
+		if (spi->mode & SPI_LOOP)
+			cr0 |= DW_HSSI_CTRLR0_SRL;
+
+		/* CTRLR0[31] MST */
+		if (dw_spi_ver_is_ge(dws, HSSI, 102A))
+			cr0 |= DW_HSSI_CTRLR0_MST;
+	}
+
+	return cr0;
+}
+
+void dw_spi_ext_update_config(struct dw_spi *dws, struct spi_device *spi,
+			  struct dw_spi_cfg *cfg)
+{
+	struct dw_spi_chip_data *chip = spi_get_ctldata(spi);
+	u32 cr0 = chip->cr0;
+	u32 speed_hz;
+	u16 clk_div;
+
+	/* CTRLR0[ 4/3: 0] or CTRLR0[ 20: 16] Data Frame Size */
+	cr0 |= (cfg->dfs - 1) << dws->dfs_offset;
+
+	if (dw_spi_ip_is(dws, PSSI))
+		/* CTRLR0[ 9:8] Transfer Mode */
+		cr0 |= FIELD_PREP(DW_PSSI_CTRLR0_TMOD_MASK, cfg->tmode);
+	else
+		/* CTRLR0[11:10] Transfer Mode */
+		cr0 |= FIELD_PREP(DW_HSSI_CTRLR0_TMOD_MASK, cfg->tmode);
+
+    if (dws->caps & DW_SPI_CAP_EXT_SPI) {
+		if (cfg->spi_frf)
+			cr0 |= FIELD_PREP(DW_HSSI_CTRLR0_SPI_FRF_MASK,
+				cfg->spi_frf);
+		else
+			cr0 &= ~DW_HSSI_CTRLR0_SPI_FRF_MASK;
+	}
+
+	dw_writel(dws, DW_SPI_CTRLR0, cr0);
+
+	if (cfg->tmode == DW_SPI_CTRLR0_TMOD_EPROMREAD ||
+	    cfg->tmode == DW_SPI_CTRLR0_TMOD_RO ||
+	    (cfg->tmode == DW_SPI_CTRLR0_TMOD_TO &&
+	     (dws->caps & DW_SPI_CAP_EXT_SPI) && cfg->spi_frf))
+		dw_writel(dws, DW_SPI_CTRLR1, cfg->ndf ? cfg->ndf - 1 : 0);
+
+	/* Note DW APB SSI clock divider doesn't support odd numbers */
+	clk_div = (DIV_ROUND_UP(dws->max_freq, cfg->freq) + 1) & 0xfffe;
+	speed_hz = dws->max_freq / clk_div;
+
+	if (dws->current_freq != speed_hz) {
+		dw_spi_set_clk(dws, clk_div);
+		dws->current_freq = speed_hz;
+	}
+
+	/* Update RX sample delay if required */
+	if (dws->cur_rx_sample_dly != chip->rx_sample_dly) {
+		dw_writel(dws, DW_SPI_RX_SAMPLE_DLY, chip->rx_sample_dly);
+		dws->cur_rx_sample_dly = chip->rx_sample_dly;
+	}
+}
+
+static void dw_spi_ext_irq_setup(struct dw_spi *dws)
+{
+	u16 level;
+	u8 imask;
+
+	/*
+	 * Originally Tx and Rx data lengths match. Rx FIFO Threshold level
+	 * will be adjusted at the final stage of the IRQ-based SPI transfer
+	 * execution so not to lose the leftover of the incoming data.
+	 */
+	level = min_t(unsigned int, dws->fifo_len / 2, dws->tx_len);
+	dw_writel(dws, DW_SPI_TXFTLR, level);
+	dw_writel(dws, DW_SPI_RXFTLR, level - 1);
+
+	dws->transfer_handler = dw_spi_transfer_handler;
+
+	imask = DW_SPI_INT_TXEI | DW_SPI_INT_TXOI |
+		DW_SPI_INT_RXUI | DW_SPI_INT_RXOI | DW_SPI_INT_RXFI;
+	dw_spi_umask_intr(dws, imask);
+}
+
+/*
+ * The iterative procedure of the poll-based transfer is simple: write as much
+ * as possible to the Tx FIFO, wait until the pending to receive data is ready
+ * to be read, read it from the Rx FIFO and check whether the performed
+ * procedure has been successful.
+ *
+ * Note this method the same way as the IRQ-based transfer won't work well for
+ * the SPI devices connected to the controller with native CS due to the
+ * automatic CS assertion/de-assertion.
+ */
+static int dw_spi_poll_transfer(struct dw_spi *dws,
+				struct spi_transfer *transfer)
+{
+	struct spi_delay delay;
+	u16 nbits;
+	int ret;
+
+	delay.unit = SPI_DELAY_UNIT_SCK;
+	nbits = dws->n_bytes * BITS_PER_BYTE;
+
+	do {
+		dw_writer(dws);
+
+		delay.value = nbits * (dws->rx_len - dws->tx_len);
+		spi_delay_exec(&delay, transfer);
+
+		dw_reader(dws);
+
+		ret = dw_spi_ext_check_status(dws, true);
+		if (ret)
+			return ret;
+	} while (dws->rx_len);
+
+	return 0;
+}
+
+static int dw_spi_transfer_one(struct spi_controller *master,
+			       struct spi_device *spi,
+			       struct spi_transfer *transfer)
+{
+	struct dw_spi *dws = spi_controller_get_devdata(master);
+	struct dw_spi_cfg cfg = {
+		.tmode = DW_SPI_CTRLR0_TMOD_TR,
+		.dfs = transfer->bits_per_word,
+		.freq = transfer->speed_hz,
+		.spi_frf = 0,
+	};
+	int ret;
+
+	dws->dma_mapped = 0;
+	dws->n_bytes = DIV_ROUND_UP(transfer->bits_per_word, BITS_PER_BYTE);
+	dws->tx = (void *)transfer->tx_buf;
+	dws->tx_len = transfer->len / dws->n_bytes;
+	dws->rx = transfer->rx_buf;
+	dws->rx_len = dws->tx_len;
+
+	/* Ensure the data above is visible for all CPUs */
+	smp_mb();
+
+	dw_spi_enable_chip(dws, 0);
+
+	dw_spi_ext_update_config(dws, spi, &cfg);
+
+	transfer->effective_speed_hz = dws->current_freq;
+
+	/* Check if current transfer is a DMA transaction */
+	if (master->can_dma && master->can_dma(master, spi, transfer))
+		dws->dma_mapped = master->cur_msg_mapped;
+
+	/* For poll mode just disable all interrupts */
+	dw_spi_mask_intr(dws, 0xff);
+
+	if (dws->dma_mapped) {
+		ret = dws->dma_ops->dma_setup(dws, transfer);
+		if (ret)
+			return ret;
+	}
+
+	dw_spi_enable_chip(dws, 1);
+
+	if (dws->dma_mapped)
+		return dws->dma_ops->dma_transfer(dws, transfer);
+	else if (dws->irq == IRQ_NOTCONNECTED)
+		return dw_spi_poll_transfer(dws, transfer);
+
+	dw_spi_ext_irq_setup(dws);
+
+	return 1;
+}
+
+static void dw_spi_handle_err(struct spi_controller *master,
+			      struct spi_message *msg)
+{
+	struct dw_spi *dws = spi_controller_get_devdata(master);
+
+	if (dws->dma_mapped)
+		dws->dma_ops->dma_stop(dws);
+
+	dw_spi_reset_chip(dws);
+}
+
+static int dw_spi_adjust_mem_op_size(struct spi_mem *mem, struct spi_mem_op *op)
+{
+	if (op->data.dir == SPI_MEM_DATA_IN)
+		op->data.nbytes = clamp_val(op->data.nbytes, 0, DW_SPI_NDF_MASK + 1);
+
+	return 0;
+}
+
+static bool dw_spi_supports_mem_op(struct spi_mem *mem,
+				   const struct spi_mem_op *op)
+{
+	struct dw_spi *dws = spi_controller_get_devdata(mem->spi->controller);
+
+	/*
+	 * Only support the numbler of io lines used to transfer the cmd 
+	 * is 1 in enhanced SPI for now.
+	 */
+	if (op->cmd.buswidth > 1)
+		return false;
+
+	/* In enhanced SPI 1, 2, 4, 8 all are valid modes. */
+	if (op->data.buswidth > 1 && (!(dws->caps & DW_SPI_CAP_EXT_SPI)))
+		return false;
+
+	/* Only support upto 60 bit address in enhanced SPI for now. */
+	if (op->data.buswidth > 1 && op->addr.nbytes > 8)
+ 		return false;
+
+	return spi_mem_default_supports_op(mem, op);
+}
+
+static int dw_spi_init_mem_buf(struct dw_spi *dws, const struct spi_mem_op *op,
+								bool enhanced_spi)
+{
+	unsigned int i, j, len;
+	u8 *out;
+
+	/*
+	 * Calculate the total length of the EEPROM command transfer and
+	 * either use the pre-allocated buffer or create a temporary one.
+	 */
+	len = op->cmd.nbytes + op->addr.nbytes + op->dummy.nbytes;
+	if (op->data.dir == SPI_MEM_DATA_OUT)
+		len += op->data.nbytes;
+
+	if (len <= DW_SPI_BUF_SIZE) {
+		out = dws->buf;
+	} else {
+		out = kzalloc(len, GFP_KERNEL);
+		if (!out)
+			return -ENOMEM;
+	}
+
+	/*
+	 * Collect the operation code, address and dummy bytes into the single
+	 * buffer. If it's a transfer with data to be sent, also copy it into the
+	 * single buffer in order to speed the data transmission up.
+	 */
+	for (i = 0; i < op->cmd.nbytes; ++i)
+		out[i] = DW_SPI_GET_BYTE(op->cmd.opcode, op->cmd.nbytes - i - 1);
+	
+	if (enhanced_spi) {
+		/*
+		 * Fill the remaining spaces of dws->reg_io_width bytes
+		 * size register with zero for cmd.
+		 */
+		for (; i < dws->reg_io_width; ++i)
+			out[i] = 0;
+		/*
+		 * Copy the address bytes in dws->reg_io_width bytes size
+		 * register and fill remaining spaces with zero.
+		 */
+		for (j = op->addr.nbytes; j > 0; ++i, --j)
+			out[i] = DW_SPI_GET_BYTE(op->addr.val, op->addr.nbytes - j);
+		for (j = op->addr.nbytes; j < dws->reg_io_width; ++i, ++j)
+			out[i] = 0;
+	} else {
+		for (j = 0; j < op->addr.nbytes; ++i, ++j)
+			out[i] = DW_SPI_GET_BYTE(op->addr.val, op->addr.nbytes - j - 1);
+	}
+
+	if (!enhanced_spi) {
+		/*
+		 * dummy bytes are not needed in enhanced mode as
+		 * wait_cycles specified as number of SPI clock cycles
+		 * between control frames transmit and data reception
+		 * will be mentioned in enhanced spi mode.
+		 */
+		for (j = 0; j < op->dummy.nbytes; ++i, ++j)
+			out[i] = 0x0;
+	}
+
+	if (op->data.dir == SPI_MEM_DATA_OUT)
+		memcpy(&out[i], op->data.buf.out, op->data.nbytes);
+
+	dws->n_bytes = 1;
+	dws->tx = out;
+	
+	if (enhanced_spi) {
+		/*
+		 * In enhanced mode cmd will be one FIFO and address
+		 * will be one more FIFO.
+		 */
+		dws->tx_len = 1;
+		if (op->addr.nbytes)
+			dws->tx_len += 1;
+		if (op->data.dir == SPI_MEM_DATA_OUT)
+			dws->tx_len += op->data.nbytes;
+	} else {
+		dws->tx_len = len;
+	}
+
+	if (op->data.dir == SPI_MEM_DATA_IN) {
+		dws->rx = op->data.buf.in;
+		dws->rx_len = op->data.nbytes;
+	} else {
+		dws->rx = NULL;
+		dws->rx_len = 0;
+	}
+
+	return 0;
+}
+
+static void dw_spi_free_mem_buf(struct dw_spi *dws)
+{
+	if (dws->tx != dws->buf)
+		kfree(dws->tx);
+}
+
+static int dw_spi_write_then_read(struct dw_spi *dws, struct spi_device *spi)
+{
+	u32 room, entries, sts;
+	unsigned int len;
+	u8 *buf;
+
+	/*
+	 * At initial stage we just pre-fill the Tx FIFO in with no rush,
+	 * since native CS hasn't been enabled yet and the automatic data
+	 * transmission won't start til we do that.
+	 */
+	len = min(dws->fifo_len, dws->tx_len);
+	buf = dws->tx;
+	while (len--)
+		dw_write_io_reg(dws, DW_SPI_DR, *buf++);
+
+	/*
+	 * After setting any bit in the SER register the transmission will
+	 * start automatically. We have to keep up with that procedure
+	 * otherwise the CS de-assertion will happen whereupon the memory
+	 * operation will be pre-terminated.
+	 */
+	len = dws->tx_len - ((void *)buf - dws->tx);
+	dw_spi_ext_set_cs(spi, false);
+	while (len) {
+		entries = readl_relaxed(dws->regs + DW_SPI_TXFLR);
+		if (!entries) {
+			dev_err(&dws->master->dev, "CS de-assertion on Tx\n");
+			return -EIO;
+		}
+		room = min(dws->fifo_len - entries, len);
+		for (; room; --room, --len)
+			dw_write_io_reg(dws, DW_SPI_DR, *buf++);
+	}
+
+	/*
+	 * Data fetching will start automatically if the EEPROM-read mode is
+	 * activated. We have to keep up with the incoming data pace to
+	 * prevent the Rx FIFO overflow causing the inbound data loss.
+	 */
+	len = dws->rx_len;
+	buf = dws->rx;
+	while (len) {
+		entries = readl_relaxed(dws->regs + DW_SPI_RXFLR);
+		if (!entries) {
+			sts = readl_relaxed(dws->regs + DW_SPI_RISR);
+			if (sts & DW_SPI_INT_RXOI) {
+				dev_err(&dws->master->dev, "FIFO overflow on Rx\n");
+				return -EIO;
+			}
+			continue;
+		}
+		entries = min(entries, len);
+		for (; entries; --entries, --len)
+			*buf++ = dw_read_io_reg(dws, DW_SPI_DR);
+	}
+
+	return 0;
+}
+
+static inline bool dw_spi_ctlr_busy(struct dw_spi *dws)
+{
+	return dw_readl(dws, DW_SPI_SR) & DW_SPI_SR_BUSY;
+}
+
+static int dw_spi_wait_mem_op_done(struct dw_spi *dws)
+{
+	int retry = DW_SPI_WAIT_RETRIES;
+	struct spi_delay delay;
+	unsigned long ns, us;
+	u32 nents;
+
+	nents = dw_readl(dws, DW_SPI_TXFLR);
+	ns = NSEC_PER_SEC / dws->current_freq * nents;
+	ns *= dws->n_bytes * BITS_PER_BYTE;
+	if (ns <= NSEC_PER_USEC) {
+		delay.unit = SPI_DELAY_UNIT_NSECS;
+		delay.value = ns;
+	} else {
+		us = DIV_ROUND_UP(ns, NSEC_PER_USEC);
+		delay.unit = SPI_DELAY_UNIT_USECS;
+		delay.value = clamp_val(us, 0, USHRT_MAX);
+	}
+
+	while (dw_spi_ctlr_busy(dws) && retry--)
+		spi_delay_exec(&delay, NULL);
+
+	if (retry < 0) {
+		dev_err(&dws->master->dev, "Mem op hanged up\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static void ext_transfer_delay(struct dw_spi *dws)
+{
+	struct spi_delay delay;
+	unsigned long ns, us;
+	u32 nents;
+
+	nents = dw_readl(dws, DW_SPI_TXFLR);
+	ns = NSEC_PER_SEC / dws->current_freq * nents;
+	ns *= dws->n_bytes * BITS_PER_BYTE;
+	if (ns <= NSEC_PER_USEC) {
+		delay.unit = SPI_DELAY_UNIT_NSECS;
+		delay.value = ns;
+	} else {
+		us = DIV_ROUND_UP(ns, NSEC_PER_USEC);
+		delay.unit = SPI_DELAY_UNIT_USECS;
+		delay.value = clamp_val(us, 0, USHRT_MAX);
+	}
+	/* wait until there is some space in TX FIFO */
+	while (!(dw_readl(dws, DW_SPI_SR) & DW_SPI_SR_TF_NOT_FULL))
+		spi_delay_exec(&delay, NULL);
+}
+
+static void dw_spi_stop_mem_op(struct dw_spi *dws, struct spi_device *spi)
+{
+	dw_spi_enable_chip(dws, 0);
+	dw_spi_ext_set_cs(spi, true);
+	dw_spi_enable_chip(dws, 1);
+}
+
+static int enhanced_transfer(struct dw_spi *dws, struct spi_device *spi,
+			     const struct spi_mem_op *op)
+{
+	u32 max, txw = 0, rxw;
+	bool cs_done = false;
+	void *buf = dws->tx;
+	int ret;
+
+	/* Send cmd as 32 bit value */
+	if (buf) {
+		txw = *(u32 *)(buf);
+		dw_write_io_reg(dws, DW_SPI_DR, txw);
+		buf += 4;
+		dws->tx_len--;
+		if (op->addr.nbytes) {
+			/*
+			 * Send address as 32 bit value if address
+			 * is present in the instruction.
+			 */
+			txw = *(u32 *)(buf);
+			dw_write_io_reg(dws, DW_SPI_DR, txw);
+			buf += 4;
+			dws->tx_len--;
+		}
+	}
+
+	do {
+		max = min_t(u32, dws->tx_len, dws->fifo_len -
+			    dw_readl(dws, DW_SPI_TXFLR));
+		while (max--) {
+			if (buf) {
+				txw = *(u8 *)(buf);
+				buf += dws->n_bytes;
+			}
+			dw_write_io_reg(dws, DW_SPI_DR, txw);
+			--dws->tx_len;
+		}
+		/* Enable CS after filling up FIFO */
+		if (!cs_done) {
+			dw_spi_ext_set_cs(spi, false);
+			cs_done = true;
+		}
+		ext_transfer_delay(dws);
+		if (!dws->tx_len && !dws->rx_len) {
+			/*
+			 * We only need to wait for done if there is
+			 * nothing to receive and there is nothing more
+			 * to transmit. If we are receiving, then the
+			 * wait cycles will make sure we wait.
+			 */
+			ret = dw_spi_wait_mem_op_done(dws);
+			if (ret)
+				return ret;
+		}
+	} while (dws->tx_len);
+
+	buf = dws->rx;
+	while (dws->rx_len) {
+		max = dw_spi_rx_max(dws);
+
+		while (max--) {
+			rxw = dw_read_io_reg(dws, DW_SPI_DR);
+			if (buf) {
+				*(u8 *)(buf) = rxw;
+				buf += dws->n_bytes;
+			}
+			--dws->rx_len;
+		}
+
+		ret = dw_spi_ext_check_status(dws, true);
+		if (ret)
+			return ret;
+	}
+	return 0;
+}
+
+static void update_spi_ctrl0(struct dw_spi *dws, const struct spi_mem_op *op, bool enable)
+{
+	u32 spi_ctrlr0;
+
+	spi_ctrlr0 = dw_readl(dws, DW_SPI_SPI_CTRLR0);
+	if (enable) {
+		spi_ctrlr0 |= FIELD_PREP(DW_HSSI_SPI_CTRLR0_WAIT_CYCLE_MASK,
+					 op->dummy.nbytes * BITS_PER_BYTE);
+		/* 8 bit instruction length */
+		spi_ctrlr0 |= FIELD_PREP(DW_HSSI_SPI_CTRLR0_INST_L_MASK,
+					 DW_HSSI_SPI_CTRLR0_INST_L8);
+		/* 32 bit address length */
+		spi_ctrlr0 |= FIELD_PREP(DW_HSSI_SPI_CTRLR0_ADDR_L_MASK,
+					 DW_HSSI_SPI_CTRLR0_ADDR_L32);
+		/* Enable clock stretching */
+		spi_ctrlr0 |= DW_HSSI_SPI_CTRLR0_CLK_STRETCH_EN;
+	} else {
+		spi_ctrlr0 &= ~DW_HSSI_SPI_CTRLR0_WAIT_CYCLE_MASK;
+		spi_ctrlr0 &= ~DW_HSSI_SPI_CTRLR0_INST_L_MASK;
+		spi_ctrlr0 &= ~DW_HSSI_SPI_CTRLR0_ADDR_L_MASK;
+		spi_ctrlr0 &= ~DW_HSSI_SPI_CTRLR0_CLK_STRETCH_EN;
+	}
+
+	dw_writel(dws, DW_SPI_SPI_CTRLR0, spi_ctrlr0);
+}
+
+/*
+ * The SPI memory operation implementation below is the best choice for the
+ * devices, which are selected by the native chip-select lane. It's
+ * specifically developed to workaround the problem with automatic chip-select
+ * lane toggle when there is no data in the Tx FIFO buffer. Luckily the current
+ * SPI-mem core calls exec_op() callback only if the GPIO-based CS is
+ * unavailable.
+ */
+static int dw_spi_exec_mem_op(struct spi_mem *mem, const struct spi_mem_op *op)
+{
+	struct dw_spi *dws = spi_controller_get_devdata(mem->spi->controller);
+	bool enhanced_spi = false;
+	struct dw_spi_cfg cfg;
+	unsigned long flags;
+	int ret;
+
+    if (dws->caps & DW_SPI_CAP_EXT_SPI) {
+		switch (op->data.buswidth) {
+		case 2:
+			cfg.spi_frf = DW_SSI_CTRLR0_SPI_FRF_DUAL_SPI;
+			enhanced_spi = true;
+			break;
+		case 4:
+			cfg.spi_frf = DW_SSI_CTRLR0_SPI_FRF_QUAD_SPI;
+			enhanced_spi = true;
+			break;
+		case 8:
+			cfg.spi_frf = DW_SSI_CTRLR0_SPI_FRF_OCT_SPI;
+			enhanced_spi = true;
+			break;
+		default:
+			cfg.spi_frf = 0;
+			break;
+		}
+	}
+
+	/*
+	 * Collect the outbound data into a single buffer to speed the
+	 * transmission up at least on the initial stage.
+	 */
+	ret = dw_spi_init_mem_buf(dws, op, enhanced_spi);
+	if (ret)
+		return ret;
+
+	/*
+	 * DW SPI EEPROM-read mode is required only for the SPI memory Data-IN
+	 * operation. Transmit-only mode is suitable for the rest of them.
+	 */
+	cfg.dfs = 8;
+	cfg.freq = clamp(mem->spi->max_speed_hz, 0U, dws->max_mem_freq);
+	if (op->data.dir == SPI_MEM_DATA_IN) {
+		if (enhanced_spi)
+			cfg.tmode = DW_SPI_CTRLR0_TMOD_RO;
+		else
+			cfg.tmode = DW_SPI_CTRLR0_TMOD_EPROMREAD;
+		cfg.ndf = op->data.nbytes;
+	} else {
+		cfg.tmode = DW_SPI_CTRLR0_TMOD_TO;
+		if (enhanced_spi)
+			cfg.ndf = op->data.nbytes;
+	}
+
+	dw_spi_enable_chip(dws, 0);
+
+    if (dws->caps & DW_SPI_CAP_EXT_SPI)
+		update_spi_ctrl0(dws, op, enhanced_spi);
+
+	dw_spi_ext_update_config(dws, mem->spi, &cfg);
+
+	dw_spi_mask_intr(dws, 0xff);
+
+	dw_spi_enable_chip(dws, 1);
+
+	/*
+	 * DW APB SSI controller has very nasty peculiarities. First originally
+	 * (without any vendor-specific modifications) it doesn't provide a
+	 * direct way to set and clear the native chip-select signal. Instead
+	 * the controller asserts the CS lane if Tx FIFO isn't empty and a
+	 * transmission is going on, and automatically de-asserts it back to
+	 * the high level if the Tx FIFO doesn't have anything to be pushed
+	 * out. Due to that a multi-tasking or heavy IRQs activity might be
+	 * fatal, since the transfer procedure preemption may cause the Tx FIFO
+	 * getting empty and sudden CS de-assertion, which in the middle of the
+	 * transfer will most likely cause the data loss. Secondly the
+	 * EEPROM-read or Read-only DW SPI transfer modes imply the incoming
+	 * data being automatically pulled in into the Rx FIFO. So if the
+	 * driver software is late in fetching the data from the FIFO before
+	 * it's overflown, new incoming data will be lost. In order to make
+	 * sure the executed memory operations are CS-atomic and to prevent the
+	 * Rx FIFO overflow we have to disable the local interrupts so to block
+	 * any preemption during the subsequent IO operations.
+	 *
+	 * Note. At some circumstances disabling IRQs may not help to prevent
+	 * the problems described above. The CS de-assertion and Rx FIFO
+	 * overflow may still happen due to the relatively slow system bus or
+	 * CPU not working fast enough, so the write-then-read algo implemented
+	 * here just won't keep up with the SPI bus data transfer. Such
+	 * situation is highly platform specific and is supposed to be fixed by
+	 * manually restricting the SPI bus frequency using the
+	 * dws->max_mem_freq parameter.
+	 */
+	if (!enhanced_spi) {
+	    local_irq_save(flags);
+	    preempt_disable();
+
+	    ret = dw_spi_write_then_read(dws, mem->spi);
+
+	    local_irq_restore(flags);
+	    preempt_enable();
+
+	    /*
+	     * Wait for the operation being finished and check the controller
+	     * status only if there hasn't been any run-time error detected. In the
+	     * former case it's just pointless. In the later one to prevent an
+	     * additional error message printing since any hw error flag being set
+	     * would be due to an error detected on the data transfer.
+	     */
+	    if (!ret) {
+		    ret = dw_spi_wait_mem_op_done(dws);
+		    if (!ret)
+			    ret = dw_spi_ext_check_status(dws, true);
+	    }
+	} else {
+		/*
+		 * We donot need to disable IRQs as clock stretching will
+		 * be enabled in enhanced mode which will prevent CS
+		 * from being de-assert.
+		 */
+		ret = enhanced_transfer(dws, mem->spi, op);
+	}
+
+	dw_spi_stop_mem_op(dws, mem->spi);
+
+	dw_spi_free_mem_buf(dws);
+
+	return ret;
+}
+
+/*
+ * Initialize the default memory operations if a glue layer hasn't specified
+ * custom ones. Direct mapping operations will be preserved anyway since DW SPI
+ * controller doesn't have an embedded dirmap interface. Note the memory
+ * operations implemented in this driver is the best choice only for the DW APB
+ * SSI controller with standard native CS functionality. If a hardware vendor
+ * has fixed the automatic CS assertion/de-assertion peculiarity, then it will
+ * be safer to use the normal SPI-messages-based transfers implementation.
+ */
+static void dw_spi_init_mem_ops(struct dw_spi *dws)
+{
+	if (!dws->mem_ops.exec_op && !(dws->caps & DW_SPI_CAP_CS_OVERRIDE) &&
+	    !dws->set_cs) {
+		dws->mem_ops.adjust_op_size = dw_spi_adjust_mem_op_size;
+		dws->mem_ops.supports_op = dw_spi_supports_mem_op;
+		dws->mem_ops.exec_op = dw_spi_exec_mem_op;
+		if (!dws->max_mem_freq)
+			dws->max_mem_freq = dws->max_freq;
+	}
+}
+
+/* This may be called twice for each spi dev */
+static int dw_spi_setup(struct spi_device *spi)
+{
+	struct dw_spi *dws = spi_controller_get_devdata(spi->controller);
+	struct dw_spi_chip_data *chip;
+
+	/* Only alloc on first setup */
+	chip = spi_get_ctldata(spi);
+	if (!chip) {
+		struct dw_spi *dws = spi_controller_get_devdata(spi->controller);
+		u32 rx_sample_dly_ns;
+
+		chip = kzalloc(sizeof(*chip), GFP_KERNEL);
+		if (!chip)
+			return -ENOMEM;
+		spi_set_ctldata(spi, chip);
+		/* Get specific / default rx-sample-delay */
+		if (device_property_read_u32(&spi->dev,
+					     "rx-sample-delay-ns",
+					     &rx_sample_dly_ns) != 0)
+			/* Use default controller value */
+			rx_sample_dly_ns = dws->def_rx_sample_dly_ns;
+		chip->rx_sample_dly = DIV_ROUND_CLOSEST(rx_sample_dly_ns,
+							NSEC_PER_SEC /
+							dws->max_freq);
+	}
+
+	/*
+	 * Update CR0 data each time the setup callback is invoked since
+	 * the device parameters could have been changed, for instance, by
+	 * the MMC SPI driver or something else.
+	 */
+	chip->cr0 = dw_spi_prepare_cr0(dws, spi);
+
+	return 0;
+}
+
+static void dw_spi_cleanup(struct spi_device *spi)
+{
+	struct dw_spi_chip_data *chip = spi_get_ctldata(spi);
+
+	kfree(chip);
+	spi_set_ctldata(spi, NULL);
+}
+
+/* Restart the controller, disable all interrupts, clean rx fifo */
+static void dw_spi_hw_init(struct device *dev, struct dw_spi *dws)
+{
+	dw_spi_reset_chip(dws);
+
+	/*
+	 * Retrieve the Synopsys component version if it hasn't been specified
+	 * by the platform. CoreKit version ID is encoded as a 3-chars ASCII
+	 * code enclosed with '*' (typical for the most of Synopsys IP-cores).
+	 */
+	if (!dws->ver) {
+		dws->ver = dw_readl(dws, DW_SPI_VERSION);
+
+		dev_dbg(dev, "Synopsys DWC%sSSI v%c.%c%c\n",
+			dw_spi_ip_is(dws, PSSI) ? " APB " : " ",
+			DW_SPI_GET_BYTE(dws->ver, 3), DW_SPI_GET_BYTE(dws->ver, 2),
+			DW_SPI_GET_BYTE(dws->ver, 1));
+	}
+
+	/*
+	 * Try to detect the FIFO depth if not set by interface driver,
+	 * the depth could be from 2 to 256 from HW spec
+	 */
+	if (!dws->fifo_len) {
+		u32 fifo;
+
+		for (fifo = 1; fifo < 256; fifo++) {
+			dw_writel(dws, DW_SPI_TXFTLR, fifo);
+			if (fifo != dw_readl(dws, DW_SPI_TXFTLR))
+				break;
+		}
+		dw_writel(dws, DW_SPI_TXFTLR, 0);
+
+		dws->fifo_len = (fifo == 1) ? 0 : fifo;
+		dev_dbg(dev, "Detected FIFO size: %u bytes\n", dws->fifo_len);
+	}
+
+	/*
+	 * Detect CTRLR0.DFS field size and offset by testing the lowest bits
+	 * writability. Note DWC SSI controller also has the extended DFS, but
+	 * with zero offset.
+	 */
+	if (dw_spi_ip_is(dws, PSSI)) {
+		u32 cr0, tmp = dw_readl(dws, DW_SPI_CTRLR0);
+
+		dw_spi_enable_chip(dws, 0);
+		dw_writel(dws, DW_SPI_CTRLR0, 0xffffffff);
+		cr0 = dw_readl(dws, DW_SPI_CTRLR0);
+		dw_writel(dws, DW_SPI_CTRLR0, tmp);
+		dw_spi_enable_chip(dws, 1);
+
+		if (!(cr0 & DW_PSSI_CTRLR0_DFS_MASK)) {
+			dws->caps |= DW_SPI_CAP_DFS32;
+			dws->dfs_offset = __bf_shf(DW_PSSI_CTRLR0_DFS32_MASK);
+			dev_dbg(dev, "Detected 32-bits max data frame size\n");
+		}
+	} else {
+		dws->caps |= DW_SPI_CAP_DFS32;
+	}
+}
+
+int dw_spi_ext_add_host(struct device *dev, struct dw_spi *dws)
+{
+	struct spi_controller *master;
+	int ret;
+
+	if (!dws)
+		return -EINVAL;
+
+	master = spi_alloc_master(dev, 0);
+	if (!master)
+		return -ENOMEM;
+
+	device_set_node(&master->dev, dev_fwnode(dev));
+
+	dws->master = master;
+	dws->dma_addr = (dma_addr_t)(dws->paddr + DW_SPI_DR);
+
+	spi_controller_set_devdata(master, dws);
+
+	/* Basic HW init */
+	dw_spi_hw_init(dev, dws);
+
+	ret = request_irq(dws->irq, dw_spi_ext_irq, IRQF_SHARED, dev_name(dev),
+			  master);
+	if (ret < 0 && ret != -ENOTCONN) {
+		dev_err(dev, "can not get IRQ\n");
+		goto err_free_master;
+	}
+
+	dw_spi_init_mem_ops(dws);
+
+	master->use_gpio_descriptors = true;
+	master->mode_bits = SPI_CPOL | SPI_CPHA | SPI_LOOP;
+	if (dws->caps & DW_SPI_CAP_EXT_SPI)
+		master->mode_bits |= SPI_TX_DUAL | SPI_RX_DUAL |
+				SPI_TX_QUAD | SPI_RX_QUAD |
+				SPI_TX_OCTAL | SPI_RX_OCTAL;
+	if (dws->caps & DW_SPI_CAP_DFS32)
+		master->bits_per_word_mask = SPI_BPW_RANGE_MASK(4, 32);
+	else
+		master->bits_per_word_mask = SPI_BPW_RANGE_MASK(4, 16);
+	master->bus_num = dws->bus_num;
+	master->num_chipselect = dws->num_cs;
+	master->setup = dw_spi_setup;
+	master->cleanup = dw_spi_cleanup;
+	if (dws->set_cs)
+		master->set_cs = dws->set_cs;
+	else
+		master->set_cs = dw_spi_ext_set_cs;
+	master->transfer_one = dw_spi_transfer_one;
+	master->handle_err = dw_spi_handle_err;
+	if (dws->mem_ops.exec_op)
+		master->mem_ops = &dws->mem_ops;
+	master->max_speed_hz = dws->max_freq;
+	master->flags = SPI_MASTER_GPIO_SS;
+	master->auto_runtime_pm = true;
+
+	/* Get default rx sample delay */
+	device_property_read_u32(dev, "rx-sample-delay-ns",
+				 &dws->def_rx_sample_dly_ns);
+
+	if (dws->dma_ops && dws->dma_ops->dma_init) {
+		ret = dws->dma_ops->dma_init(dev, dws);
+		if (ret == -EPROBE_DEFER) {
+			goto err_free_irq;
+		} else if (ret) {
+			dev_warn(dev, "DMA init failed\n");
+		} else {
+			master->can_dma = dws->dma_ops->can_dma;
+			master->flags |= SPI_CONTROLLER_MUST_TX;
+		}
+	}
+
+	ret = spi_register_controller(master);
+	if (ret) {
+		dev_err_probe(dev, ret, "problem registering spi master\n");
+		goto err_dma_exit;
+	}
+
+	dw_spi_debugfs_init(dws);
+	return 0;
+
+err_dma_exit:
+	if (dws->dma_ops && dws->dma_ops->dma_exit)
+		dws->dma_ops->dma_exit(dws);
+	dw_spi_enable_chip(dws, 0);
+err_free_irq:
+	free_irq(dws->irq, master);
+err_free_master:
+	spi_controller_put(master);
+	return ret;
+}
+
+void dw_spi_ext_remove_host(struct dw_spi *dws)
+{
+	dw_spi_debugfs_remove(dws);
+
+	spi_unregister_controller(dws->master);
+
+	if (dws->dma_ops && dws->dma_ops->dma_exit)
+		dws->dma_ops->dma_exit(dws);
+
+	dw_spi_shutdown_chip(dws);
+
+	free_irq(dws->irq, dws->master);
+}
+
+int dw_spi_ext_suspend_host(struct dw_spi *dws)
+{
+	int ret;
+
+	ret = spi_controller_suspend(dws->master);
+	if (ret)
+		return ret;
+
+	dw_spi_shutdown_chip(dws);
+	return 0;
+}
+
+int dw_spi_ext_resume_host(struct dw_spi *dws)
+{
+	dw_spi_hw_init(&dws->master->dev, dws);
+	return spi_controller_resume(dws->master);
+}
+
+MODULE_AUTHOR("George hu");
+MODULE_DESCRIPTION("Spacemit enhance spi driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/spi/spi-dw-espi.h b/drivers/spi/spi-dw-espi.h
new file mode 100755
index 000000000000..f9006bf408d6
--- /dev/null
+++ b/drivers/spi/spi-dw-espi.h
@@ -0,0 +1,325 @@
+// SPDX-License-Identifier: GPL-2.0-only
+
+#ifndef DW_ESPI_HEADER_H
+#define DW_ESPI_HEADER_H
+
+#include <linux/bits.h>
+#include <linux/completion.h>
+#include <linux/debugfs.h>
+#include <linux/irqreturn.h>
+#include <linux/io.h>
+#include <linux/scatterlist.h>
+#include <linux/spi/spi-mem.h>
+#include <linux/bitfield.h>
+
+/* Synopsys DW SSI IP-core virtual IDs */
+#define DW_PSSI_ID			0
+#define DW_HSSI_ID			1
+
+/* Synopsys DW SSI component versions (FourCC sequence) */
+#define DW_HSSI_102A			0x3130322a
+
+/* DW SSI IP-core ID and version check helpers */
+#define dw_spi_ip_is(_dws, _ip) \
+	((_dws)->ip == DW_ ## _ip ## _ID)
+
+#define __dw_spi_ver_cmp(_dws, _ip, _ver, _op) \
+	(dw_spi_ip_is(_dws, _ip) && (_dws)->ver _op DW_ ## _ip ## _ ## _ver)
+
+#define dw_spi_ver_is(_dws, _ip, _ver) __dw_spi_ver_cmp(_dws, _ip, _ver, ==)
+
+#define dw_spi_ver_is_ge(_dws, _ip, _ver) __dw_spi_ver_cmp(_dws, _ip, _ver, >=)
+
+/* DW SPI controller capabilities */
+#define DW_SPI_CAP_CS_OVERRIDE		BIT(0)
+#define DW_SPI_CAP_DFS32		BIT(1)
+#define DW_SPI_CAP_EXT_SPI		BIT(2)
+
+/* Register offsets (Generic for both DWC APB SSI and DWC SSI IP-cores) */
+#define DW_SPI_CTRLR0			0x00
+#define DW_SPI_CTRLR1			0x04
+#define DW_SPI_SSIENR			0x08
+#define DW_SPI_MWCR			0x0c
+#define DW_SPI_SER			0x10
+#define DW_SPI_BAUDR			0x14
+#define DW_SPI_TXFTLR			0x18
+#define DW_SPI_RXFTLR			0x1c
+#define DW_SPI_TXFLR			0x20
+#define DW_SPI_RXFLR			0x24
+#define DW_SPI_SR			0x28
+#define DW_SPI_IMR			0x2c
+#define DW_SPI_ISR			0x30
+#define DW_SPI_RISR			0x34
+#define DW_SPI_TXOICR			0x38
+#define DW_SPI_RXOICR			0x3c
+#define DW_SPI_RXUICR			0x40
+#define DW_SPI_MSTICR			0x44
+#define DW_SPI_ICR			0x48
+#define DW_SPI_DMACR			0x4c
+#define DW_SPI_DMATDLR			0x50
+#define DW_SPI_DMARDLR			0x54
+#define DW_SPI_IDR			0x58
+#define DW_SPI_VERSION			0x5c
+#define DW_SPI_DR			0x60
+#define DW_SPI_RX_SAMPLE_DLY		0xf0
+#define DW_SPI_SPI_CTRLR0		0xf4
+#define DW_SPI_DDR_DRV_EDGE     0xf8
+
+/* Bit fields in CTRLR0 (DWC APB SSI) */
+#define DW_PSSI_CTRLR0_DFS_MASK			GENMASK(3, 0)
+#define DW_PSSI_CTRLR0_DFS32_MASK		GENMASK(20, 16)
+
+#define DW_PSSI_CTRLR0_FRF_MASK			GENMASK(5, 4)
+#define DW_SPI_CTRLR0_FRF_MOTO_SPI		0x0
+#define DW_SPI_CTRLR0_FRF_TI_SSP		0x1
+#define DW_SPI_CTRLR0_FRF_NS_MICROWIRE		0x2
+#define DW_SPI_CTRLR0_FRF_RESV			0x3
+
+#define DW_PSSI_CTRLR0_MODE_MASK		GENMASK(7, 6)
+#define DW_PSSI_CTRLR0_SCPHA			BIT(6)
+#define DW_PSSI_CTRLR0_SCPOL			BIT(7)
+
+#define DW_PSSI_CTRLR0_TMOD_MASK		GENMASK(9, 8)
+#define DW_SPI_CTRLR0_TMOD_TR			0x0	/* xmit & recv */
+#define DW_SPI_CTRLR0_TMOD_TO			0x1	/* xmit only */
+#define DW_SPI_CTRLR0_TMOD_RO			0x2	/* recv only */
+#define DW_SPI_CTRLR0_TMOD_EPROMREAD		0x3	/* eeprom read mode */
+
+#define DW_PSSI_CTRLR0_SLV_OE			BIT(10)
+#define DW_PSSI_CTRLR0_SRL			BIT(11)
+#define DW_PSSI_CTRLR0_CFS			BIT(12)
+
+/* Bit fields in CTRLR0 (DWC SSI with AHB interface) */
+#define DW_HSSI_CTRLR0_DFS_MASK			GENMASK(4, 0)
+#define DW_HSSI_CTRLR0_FRF_MASK			GENMASK(7, 6)
+#define DW_HSSI_CTRLR0_SCPHA			BIT(8)
+#define DW_HSSI_CTRLR0_SCPOL			BIT(9)
+#define DW_HSSI_CTRLR0_TMOD_MASK		GENMASK(11, 10)
+#define DW_HSSI_CTRLR0_SRL			BIT(13)
+#define DW_HSSI_CTRLR0_MST			BIT(31)
+
+/* Bit fields in CTRLR0 for enhanced SPI */
+#define DW_HSSI_CTRLR0_SPI_FRF_MASK		GENMASK(23, 22)
+#define DW_SSI_CTRLR0_SPI_FRF_DUAL_SPI		0x1
+#define DW_SSI_CTRLR0_SPI_FRF_QUAD_SPI		0x2
+#define DW_SSI_CTRLR0_SPI_FRF_OCT_SPI		0x3
+
+/* Bit fields in CTRLR1 */
+#define DW_SPI_NDF_MASK				GENMASK(15, 0)
+
+/* Bit fields in SR, 7 bits */
+#define DW_SPI_SR_MASK				GENMASK(6, 0)
+#define DW_SPI_SR_BUSY				BIT(0)
+#define DW_SPI_SR_TF_NOT_FULL			BIT(1)
+#define DW_SPI_SR_TF_EMPT			BIT(2)
+#define DW_SPI_SR_RF_NOT_EMPT			BIT(3)
+#define DW_SPI_SR_RF_FULL			BIT(4)
+#define DW_SPI_SR_TX_ERR			BIT(5)
+#define DW_SPI_SR_DCOL				BIT(6)
+
+/* Bit fields in ISR, IMR, RISR, 7 bits */
+#define DW_SPI_INT_MASK				GENMASK(5, 0)
+#define DW_SPI_INT_TXEI				BIT(0)
+#define DW_SPI_INT_TXOI				BIT(1)
+#define DW_SPI_INT_RXUI				BIT(2)
+#define DW_SPI_INT_RXOI				BIT(3)
+#define DW_SPI_INT_RXFI				BIT(4)
+#define DW_SPI_INT_MSTI				BIT(5)
+
+/* Bit fields in DMACR */
+#define DW_SPI_DMACR_RDMAE			BIT(0)
+#define DW_SPI_DMACR_TDMAE			BIT(1)
+
+/* Bit fields in SPI_CTRLR0 (Defined in DWC SSI >= 1.02a) */
+#define DW_HSSI_SPI_CTRLR0_CLK_STRETCH_EN	BIT(30)
+#define DW_HSSI_SPI_CTRLR0_WAIT_CYCLE_MASK	GENMASK(15, 11)
+#define DW_HSSI_SPI_CTRLR0_INST_L_MASK		GENMASK(9, 8)
+#define DW_HSSI_SPI_CTRLR0_INST_L8		0x2
+#define DW_HSSI_SPI_CTRLR0_ADDR_L_MASK		GENMASK(5, 2)
+#define DW_HSSI_SPI_CTRLR0_ADDR_L32		0x8
+
+/* Mem/DMA operations helpers */
+#define DW_SPI_WAIT_RETRIES			5
+#define DW_SPI_BUF_SIZE \
+	(sizeof_field(struct spi_mem_op, cmd.opcode) + \
+	 sizeof_field(struct spi_mem_op, addr.val) + 256)
+#define DW_SPI_GET_BYTE(_val, _idx) \
+	((_val) >> (BITS_PER_BYTE * (_idx)) & 0xff)
+
+/* Slave spi_transfer/spi_mem_op related */
+struct dw_spi_cfg {
+	u8 tmode;
+	u8 dfs;
+	u32 ndf;
+	u32 freq;
+	u8 spi_frf;
+};
+
+struct dw_spi;
+struct dw_spi_dma_ops {
+	int (*dma_init)(struct device *dev, struct dw_spi *dws);
+	void (*dma_exit)(struct dw_spi *dws);
+	int (*dma_setup)(struct dw_spi *dws, struct spi_transfer *xfer);
+	bool (*can_dma)(struct spi_controller *master, struct spi_device *spi,
+			struct spi_transfer *xfer);
+	int (*dma_transfer)(struct dw_spi *dws, struct spi_transfer *xfer);
+	void (*dma_stop)(struct dw_spi *dws);
+};
+
+struct dw_spi {
+	struct spi_controller	*master;
+
+	u32			ip;		/* Synopsys DW SSI IP-core ID */
+	u32			ver;		/* Synopsys component version */
+	u32			caps;		/* DW SPI capabilities */
+
+	void __iomem		*regs;
+	unsigned long		paddr;
+	int			irq;
+	u32			fifo_len;	/* depth of the FIFO buffer */
+	unsigned int		dfs_offset;     /* CTRLR0 DFS field offset */
+	u32			max_mem_freq;	/* max mem-ops bus freq */
+	u32			max_freq;	/* max bus freq supported */
+
+	u32			reg_io_width;	/* DR I/O width in bytes */
+	u16			bus_num;
+	u16			num_cs;		/* supported slave numbers */
+	void (*set_cs)(struct spi_device *spi, bool enable);
+
+	/* Current message transfer state info */
+	void			*tx;
+	unsigned int		tx_len;
+	void			*rx;
+	unsigned int		rx_len;
+	u8			buf[DW_SPI_BUF_SIZE];
+	int			dma_mapped;
+	u8			n_bytes;	/* current is a 1/2 bytes op */
+	irqreturn_t		(*transfer_handler)(struct dw_spi *dws);
+	u32			current_freq;	/* frequency in hz */
+	u32			cur_rx_sample_dly;
+	u32			def_rx_sample_dly_ns;
+
+	/* Custom memory operations */
+	struct spi_controller_mem_ops mem_ops;
+
+	/* DMA info */
+	struct dma_chan		*txchan;
+	u32			txburst;
+	struct dma_chan		*rxchan;
+	u32			rxburst;
+	u32			dma_sg_burst;
+	unsigned long		dma_chan_busy;
+	dma_addr_t		dma_addr; /* phy address of the Data register */
+	const struct dw_spi_dma_ops *dma_ops;
+	struct completion	dma_completion;
+
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *debugfs;
+	struct debugfs_regset32 regset;
+#endif
+};
+
+static inline u32 dw_readl(struct dw_spi *dws, u32 offset)
+{
+	return __raw_readl(dws->regs + offset);
+}
+
+static inline void dw_writel(struct dw_spi *dws, u32 offset, u32 val)
+{
+	__raw_writel(val, dws->regs + offset);
+}
+
+static inline u32 dw_read_io_reg(struct dw_spi *dws, u32 offset)
+{
+	switch (dws->reg_io_width) {
+	case 2:
+		return readw_relaxed(dws->regs + offset);
+	case 4:
+	default:
+		return readl_relaxed(dws->regs + offset);
+	}
+}
+
+static inline void dw_write_io_reg(struct dw_spi *dws, u32 offset, u32 val)
+{
+	switch (dws->reg_io_width) {
+	case 2:
+		writew_relaxed(val, dws->regs + offset);
+		break;
+	case 4:
+	default:
+		writel_relaxed(val, dws->regs + offset);
+		break;
+	}
+}
+
+static inline void dw_spi_enable_chip(struct dw_spi *dws, int enable)
+{
+	dw_writel(dws, DW_SPI_SSIENR, (enable ? 1 : 0));
+}
+
+static inline void dw_spi_set_clk(struct dw_spi *dws, u16 div)
+{
+	dw_writel(dws, DW_SPI_BAUDR, div);
+}
+
+/* Disable IRQ bits */
+static inline void dw_spi_mask_intr(struct dw_spi *dws, u32 mask)
+{
+	u32 new_mask;
+
+	new_mask = dw_readl(dws, DW_SPI_IMR) & ~mask;
+	dw_writel(dws, DW_SPI_IMR, new_mask);
+}
+
+/* Enable IRQ bits */
+static inline void dw_spi_umask_intr(struct dw_spi *dws, u32 mask)
+{
+	u32 new_mask;
+
+	new_mask = dw_readl(dws, DW_SPI_IMR) | mask;
+	dw_writel(dws, DW_SPI_IMR, new_mask);
+}
+
+/*
+ * This disables the SPI controller, interrupts, clears the interrupts status
+ * and CS, then re-enables the controller back. Transmit and receive FIFO
+ * buffers are cleared when the device is disabled.
+ */
+static inline void dw_spi_reset_chip(struct dw_spi *dws)
+{
+	dw_spi_enable_chip(dws, 0);
+	dw_spi_mask_intr(dws, 0xff);
+	dw_readl(dws, DW_SPI_ICR);
+	dw_writel(dws, DW_SPI_SER, 0);
+	dw_spi_enable_chip(dws, 1);
+}
+
+static inline void dw_spi_shutdown_chip(struct dw_spi *dws)
+{
+	dw_spi_enable_chip(dws, 0);
+	dw_spi_set_clk(dws, 0);
+}
+
+extern void dw_spi_ext_set_cs(struct spi_device *spi, bool enable);
+extern void dw_spi_ext_update_config(struct dw_spi *dws, struct spi_device *spi,
+				 struct dw_spi_cfg *cfg);
+extern int dw_spi_ext_check_status(struct dw_spi *dws, bool raw);
+extern int dw_spi_ext_add_host(struct device *dev, struct dw_spi *dws);
+extern void dw_spi_ext_remove_host(struct dw_spi *dws);
+extern int dw_spi_ext_suspend_host(struct dw_spi *dws);
+extern int dw_spi_ext_resume_host(struct dw_spi *dws);
+
+#ifdef CONFIG_SPI_DW_DMA
+
+extern void dw_spi_dma_setup_mfld(struct dw_spi *dws);
+extern void dw_spi_dma_setup_generic(struct dw_spi *dws);
+
+#else
+
+static inline void dw_spi_dma_setup_mfld(struct dw_spi *dws) {}
+static inline void dw_spi_dma_setup_generic(struct dw_spi *dws) {}
+
+#endif /* !CONFIG_SPI_DW_DMA */
+
+#endif /* DW_ESPI_HEADER_H */
diff --git a/drivers/spi/spi-dw-mmio-ext.c b/drivers/spi/spi-dw-mmio-ext.c
new file mode 100755
index 000000000000..ca48b551d0d8
--- /dev/null
+++ b/drivers/spi/spi-dw-mmio-ext.c
@@ -0,0 +1,171 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Memory-mapped interface driver for DW SPI Core
+ *
+ * Copyright (c) 2023, spacemit.
+ */
+
+#include <linux/clk.h>
+#include <linux/err.h>
+#include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
+#include <linux/slab.h>
+#include <linux/spi/spi.h>
+#include <linux/scatterlist.h>
+#include <linux/mfd/syscon.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/acpi.h>
+#include <linux/property.h>
+#include <linux/regmap.h>
+#include <linux/reset.h>
+#include "spi-dw-espi.h"
+
+#define DRIVER_NAME "dw_spi_mmio_ext"
+
+struct dw_spi_mmio_ext {
+	struct dw_spi  dws;
+	struct clk     *clk;
+	struct clk     *pclk;
+	void           *priv;
+	struct reset_control *rstc;
+};
+
+static int dw_spi_hssi_ext_init(struct platform_device *pdev,
+			    struct dw_spi_mmio_ext *dwsmmio)
+{
+	dwsmmio->dws.ip = DW_HSSI_ID;
+	dwsmmio->dws.caps = DW_SPI_CAP_EXT_SPI;
+
+	dw_spi_dma_setup_generic(&dwsmmio->dws);
+
+	return 0;
+}
+
+static int dw_spi_mmio_ext_probe(struct platform_device *pdev)
+{
+	int (*init_func)(struct platform_device *pdev,
+			 struct dw_spi_mmio_ext *dwsmmio);
+	struct dw_spi_mmio_ext *dwsmmio;
+    struct resource *mem;
+	struct dw_spi *dws;
+	int ret;
+	int num_cs;
+
+	dwsmmio = devm_kzalloc(&pdev->dev, sizeof(struct dw_spi_mmio_ext),
+			GFP_KERNEL);
+	if (!dwsmmio)
+		return -ENOMEM;
+
+	dws = &dwsmmio->dws;
+
+	/* Get basic io resource and map it */
+	dws->regs = devm_platform_get_and_ioremap_resource(pdev, 0, &mem);
+	if (IS_ERR(dws->regs))
+		return PTR_ERR(dws->regs);
+
+	dws->paddr = mem->start;
+
+	dws->irq = platform_get_irq(pdev, 0);
+	if (dws->irq < 0)
+		return dws->irq; /* -ENXIO */
+
+	dwsmmio->clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(dwsmmio->clk))
+		return PTR_ERR(dwsmmio->clk);
+	ret = clk_prepare_enable(dwsmmio->clk);
+	if (ret)
+		return ret;
+
+	/* Optional clock needed to access the registers */
+	dwsmmio->pclk = devm_clk_get_optional(&pdev->dev, "pclk");
+	if (IS_ERR(dwsmmio->pclk)) {
+		ret = PTR_ERR(dwsmmio->pclk);
+		goto out_clk;
+	}
+	ret = clk_prepare_enable(dwsmmio->pclk);
+	if (ret)
+		goto out_clk;
+
+	/* find an optional reset controller */
+	dwsmmio->rstc = devm_reset_control_get_optional_exclusive(&pdev->dev, NULL);
+	if (IS_ERR(dwsmmio->rstc)) {
+		ret = PTR_ERR(dwsmmio->rstc);
+		dev_err(&pdev->dev, "failed to get reset.\n");
+		goto out_clk;
+	}
+	reset_control_deassert(dwsmmio->rstc);
+
+	/* set bus number */
+	dws->bus_num = pdev->id;
+
+	/* get supported freq in max */
+	dws->max_freq = clk_get_rate(dwsmmio->clk);
+
+	/* get reg width of controler */
+	device_property_read_u32(&pdev->dev, "reg-io-width", &dws->reg_io_width);
+
+	/* get chip select count of controler */
+	num_cs = 4;
+	device_property_read_u32(&pdev->dev, "num-cs", &num_cs);
+	dws->num_cs = num_cs;
+	init_func = device_get_match_data(&pdev->dev);
+	if (init_func) {
+		ret = init_func(pdev, dwsmmio);
+		if (ret)
+			goto out;
+	}
+
+	pm_runtime_enable(&pdev->dev);
+
+	ret = dw_spi_ext_add_host(&pdev->dev, dws);
+	if (ret)
+		goto out;
+
+	platform_set_drvdata(pdev, dwsmmio);
+	dev_info(&pdev->dev, "dw_spi_ext_probe success.\n");
+	return 0;
+
+out:
+	pm_runtime_disable(&pdev->dev);
+	clk_disable_unprepare(dwsmmio->pclk);
+out_clk:
+	clk_disable_unprepare(dwsmmio->clk);
+	reset_control_assert(dwsmmio->rstc);
+
+	return ret;
+}
+
+static int dw_spi_mmio_ext_remove(struct platform_device *pdev)
+{
+	struct dw_spi_mmio_ext *dwsmmio = platform_get_drvdata(pdev);
+
+	dw_spi_ext_remove_host(&dwsmmio->dws);
+    pm_runtime_disable(&pdev->dev);
+	clk_disable_unprepare(dwsmmio->pclk);
+	clk_disable_unprepare(dwsmmio->clk);
+	reset_control_assert(dwsmmio->rstc);
+
+	return 0;
+}
+
+static const struct of_device_id dw_spi_mmio_ext_of_match[] = {
+	{ .compatible = "snps,dwc-ssi-1.02a", .data = dw_spi_hssi_ext_init},
+	{ /* end of table */}
+};
+MODULE_DEVICE_TABLE(of, dw_spi_mmio_ext_of_match);
+
+static struct platform_driver dw_spi_mmio_ext_driver = {
+	.probe		= dw_spi_mmio_ext_probe,
+	.remove		= dw_spi_mmio_ext_remove,
+	.driver		= {
+		.name	= DRIVER_NAME,
+		.of_match_table = dw_spi_mmio_ext_of_match,
+	},
+};
+module_platform_driver(dw_spi_mmio_ext_driver);
+
+MODULE_AUTHOR("George hu");
+MODULE_DESCRIPTION("Spacemit qspi driver");
+MODULE_LICENSE("GPL v2");
-- 
2.47.0

